{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../../../data/raw/river/arno.csv')\n",
    "\n",
    "# some of our features are missing loads of datapoints;\n",
    "# the thinking here is: if the amount of rainfall at a\n",
    "# certain location hasn't been measured in 13 years,\n",
    "# are we to expect it will suddenly come into play for\n",
    "# new predictions? drop these \"legacy\" features\n",
    "df = df.drop([\n",
    "    'Rainfall_Vernio', 'Rainfall_Stia', 'Rainfall_Consuma', 'Rainfall_Incisa',\n",
    "    'Rainfall_Montevarchi', 'Rainfall_S_Savino', 'Rainfall_Laterina',\n",
    "    'Rainfall_Bibbiena', 'Rainfall_Camaldoli'\n",
    "], axis=1)\n",
    "\n",
    "# dates are in format DD/MM/YYYY\n",
    "# convert to this explicitly\n",
    "df['Date'] = pd.to_datetime(df['Date'], format='%d/%m/%Y')\n",
    "\n",
    "# we have temperature data going back to 2000,\n",
    "# but none of the rainfall data starts until 2004\n",
    "df = df[df['Date'] > datetime(2003, 12, 31)]\n",
    "\n",
    "# temperature data only goes through 2017\n",
    "df = df[df['Date'] < datetime(2017, 3, 9)]\n",
    "\n",
    "# a six month block is missing target data;\n",
    "# drop these rows as well\n",
    "df = df.drop(index=range(3835,4018))\n",
    "\n",
    "# there are still a few 0s in the target that\n",
    "# should be null; nullify them\n",
    "df['Hydrometry_Nave_di_Rosano'] = df['Hydrometry_Nave_di_Rosano'].replace(0, float('nan'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are still some missing values in our data that\n",
    "# look approximately like this:\n",
    "#\n",
    "# 1/1/2000 - 1.9\n",
    "# 1/2/2000 - 1.7\n",
    "# 1/3/2000 - NaN\n",
    "# 1/4/2000 - NaN\n",
    "# 1/5/2000 - 2.3\n",
    "# 1/6/2000 - 2.2\n",
    "#\n",
    "# The idea here is to interpolate any missing values\n",
    "# based on the nearest non-null values; so the above\n",
    "# would be imputed \"by steps\" like so:\n",
    "#\n",
    "# 1/2/2000 - 1.7\n",
    "# 1/3/2000 - 1.9 <- imputed\n",
    "# 1/4/2000 - 2.1 <- imputed\n",
    "# 1/5/2000 - 2.3\n",
    "\n",
    "def find_previous_nonnull(df, ix, col):\n",
    "    i = ix\n",
    "    while df.loc[i][col] != df.loc[i][col]:\n",
    "        i -= 1\n",
    "    return i\n",
    "\n",
    "def find_next_nonnull(df, ix, col):\n",
    "    i = ix\n",
    "    while df.loc[i][col] != df.loc[i][col]:\n",
    "        i += 1\n",
    "    return i\n",
    "\n",
    "def impute_at(df, ix, col):\n",
    "    a = find_previous_nonnull(df, ix, col)\n",
    "    b = find_next_nonnull(df, ix, col)\n",
    "    steps = b - a\n",
    "    dist = df.loc[b][col] - df.loc[a][col]\n",
    "    off = ix - a\n",
    "    df.at[ix, col] = df.loc[a][col] + off * dist / steps\n",
    "\n",
    "def impute_by_step(df, col):\n",
    "    ix = df[df[col].isnull()].index\n",
    "    for i in ix:\n",
    "        impute_at(df, i, col)\n",
    "\n",
    "impute_by_step(df, 'Hydrometry_Nave_di_Rosano')\n",
    "impute_by_step(df, 'Temperature_Firenze')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# center & standardize\n",
    "\n",
    "# we have to picky about the date, which is an invalid data type\n",
    "sansdate = df.drop('Date', axis=1)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(sansdate.values)\n",
    "df2 = pd.DataFrame(X, columns=sansdate.columns)\n",
    "\n",
    "# add the dates back in\n",
    "df2['Date'] = df['Date'].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# because our data contains a large schism, which may not\n",
    "# be easy to discern later, we'll just save it as two different datasets\n",
    "\n",
    "dfA = df2[:1643]\n",
    "dfB = df2[1644:]\n",
    "\n",
    "dfA.to_csv('../../../data/clean/river/arnoA.csv', index=False)\n",
    "dfB.to_csv('../../../data/clean/river/arnoB.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
