{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import miceforest as mf\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "General function for cleaning the data belonging to (optimistically) *any* aquifer dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_aquifers(df, X, y):\n",
    "    \n",
    "    # set date as the index\n",
    "    df['Date'] = pd.to_datetime(df['Date'], format='%d/%m/%Y')\n",
    "    df = df.set_index('Date', drop=True)\n",
    "    df.index.name = None\n",
    "    df.index.freq = 'D'\n",
    "    \n",
    "    # find the latest date of first non-null\n",
    "    # amongst our targets; throw away all data\n",
    "    # prior to this point in time\n",
    "    sent = None\n",
    "    for c in y:\n",
    "        data = df[~df[c].isnull()]\n",
    "        if data.shape[0] > 0:\n",
    "            d = data.iloc[0].name\n",
    "            if sent is None or d > sent:\n",
    "                sent = d\n",
    "    if sent is not None:\n",
    "        df = df.loc[sent:]\n",
    "\n",
    "    # similarly, find the earliest date of last non-null\n",
    "    # among the targets, again for trimming purposes\n",
    "    sent = None\n",
    "    for c in y:\n",
    "        data = df[~df[c].isnull()]\n",
    "        if data.shape[0] > 0:\n",
    "            d = data.iloc[-1].name\n",
    "            if sent is None or d < sent:\n",
    "                sent = d\n",
    "    if sent is not None:\n",
    "        df = df.loc[:sent]\n",
    "        \n",
    "    # 0 in the target always represents a missing value\n",
    "    for c in y:\n",
    "        df[c] = df[c].replace(0, float('nan'))\n",
    "\n",
    "    # discard features with >70% nulls\n",
    "    xnulls = df[X].isnull().sum() / df.shape[0]\n",
    "    Xs = df[xnulls[xnulls < 0.7].index]\n",
    "    X = Xs.columns\n",
    "    df = Xs.join(df[y])\n",
    "    \n",
    "    # take the absolute value of some columns\n",
    "    abs_starts = [\n",
    "        'Rainfall',\n",
    "        'Depth_to_Groundwater',\n",
    "        'Volume'\n",
    "    ]\n",
    "    for beg in abs_starts:\n",
    "        cols = [c for c in df.columns if c.startswith(beg)]\n",
    "        for c in cols:\n",
    "            df[c] = df[c].abs()\n",
    "\n",
    "    # multiple imputation\n",
    "    kernel = mf.MultipleImputedKernel(\n",
    "        data=df,\n",
    "        save_all_iterations=False,\n",
    "        random_state=143\n",
    "    )\n",
    "    kernel.mice(3, verbose=False)\n",
    "    df = kernel.impute_new_data(df).complete_data(0)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convenience class for loading (+ cleaning) the individual datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AquiferDataset:\n",
    "    \n",
    "    def __init__(self, path, X, y):\n",
    "        print(f\"loading {path}\")\n",
    "        \n",
    "        self.ycard = len(y)\n",
    "        self.df = pd.read_csv(path)\n",
    "        self.df = clean_aquifers(self.df, X, y)\n",
    "        self.X = self.df.columns[:-self.ycard]\n",
    "        self.y = self.df.columns[-self.ycard:]\n",
    "        \n",
    "        # center & scale the data (*fuller explanation below)\n",
    "        self.scaler = StandardScaler()\n",
    "        self.df = pd.DataFrame(\n",
    "            self.scaler.fit_transform(self.df),\n",
    "            columns=self.df.columns,\n",
    "            index=self.df.index\n",
    "        )\n",
    "        self.mu = self.scaler.mean_\n",
    "        self.sigma = self.scaler.scale_\n",
    "    \n",
    "    def unscale(self, v):\n",
    "        if len(v.shape) == 1:\n",
    "            if self.ycard != 1:\n",
    "                raise ValueError()\n",
    "            else:\n",
    "                return v * self.sigma[0] + self.mu[0]\n",
    "        elif v.shape[1] != self.ycard:\n",
    "            raise ValueError()\n",
    "        else:\n",
    "            for i, c in enumerate(v.columns):\n",
    "                v[c] = v[c] * self.sigma[i] + self.mu[i]\n",
    "            return v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\* We scale the data separately because of the following conundrum: When producing predictions, we want everything z-scored so that different datasets can be compared on the same scale; but when producing predictions, we *don't* want everything z-scored, because we're interested in actual units of measurement. We have to be able to do everything under the hood with z-scores, but still produce meaningful predictions\n",
    "\n",
    "Load each dataset and split by independent/dependent variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading ../data/raw/aquifer/auser.csv\n",
      "loading ../data/raw/aquifer/doganella.csv\n",
      "loading ../data/raw/aquifer/luco.csv\n",
      "loading ../data/raw/aquifer/petrignano.csv\n"
     ]
    }
   ],
   "source": [
    "auser = AquiferDataset(\n",
    "    '../data/raw/aquifer/auser.csv', [\n",
    "        'Rainfall_Gallicano', 'Rainfall_Pontetetto', 'Rainfall_Monte_Serra',\n",
    "        'Rainfall_Orentano', 'Rainfall_Borgo_a_Mozzano', 'Rainfall_Piaggione',\n",
    "        'Rainfall_Calavorno', 'Rainfall_Croce_Arcana', 'Rainfall_Tereglio_Coreglia_Antelminelli',\n",
    "        'Rainfall_Fabbriche_di_Vallico', 'Depth_to_Groundwater_PAG', 'Depth_to_Groundwater_DIEC',\n",
    "        'Temperature_Orentano', 'Temperature_Monte_Serra', 'Temperature_Ponte_a_Moriano',\n",
    "        'Temperature_Lucca_Orto_Botanico', 'Volume_POL', 'Volume_CC1', 'Volume_CC2',\n",
    "        'Volume_CSA', 'Volume_CSAL', 'Hydrometry_Monte_S_Quirico', 'Hydrometry_Piaggione'\n",
    "    ], [\n",
    "        'Depth_to_Groundwater_SAL', 'Depth_to_Groundwater_CoS', 'Depth_to_Groundwater_LT2'\n",
    "    ])\n",
    "\n",
    "doganella = AquiferDataset(\n",
    "    '../data/raw/aquifer/doganella.csv', [\n",
    "        'Rainfall_Monteporzio', 'Rainfall_Velletri', 'Volume_Pozzo_1',\n",
    "        'Volume_Pozzo_2', 'Volume_Pozzo_3', 'Volume_Pozzo_4', 'Volume_Pozzo_5+6',\n",
    "        'Volume_Pozzo_7', 'Volume_Pozzo_8', 'Volume_Pozzo_9', 'Temperature_Monteporzio',\n",
    "        'Temperature_Velletri'\n",
    "    ], [\n",
    "        'Depth_to_Groundwater_Pozzo_1', 'Depth_to_Groundwater_Pozzo_2',\n",
    "        'Depth_to_Groundwater_Pozzo_3', 'Depth_to_Groundwater_Pozzo_4',\n",
    "        'Depth_to_Groundwater_Pozzo_5', 'Depth_to_Groundwater_Pozzo_6',\n",
    "        'Depth_to_Groundwater_Pozzo_7', 'Depth_to_Groundwater_Pozzo_8',\n",
    "        'Depth_to_Groundwater_Pozzo_9'\n",
    "    ])\n",
    "\n",
    "luco = AquiferDataset(\n",
    "    '../data/raw/aquifer/luco.csv', [\n",
    "        'Rainfall_Simignano', 'Rainfall_Siena_Poggio_al_Vento', 'Rainfall_Mensano',\n",
    "        'Rainfall_Montalcinello', 'Rainfall_Monticiano_la_Pineta', 'Rainfall_Sovicille',\n",
    "        'Rainfall_Ponte_Orgia', 'Rainfall_Scorgiano', 'Rainfall_Pentolina',\n",
    "        'Rainfall_Monteroni_Arbia_Biena', 'Depth_to_Groundwater_Pozzo_1',\n",
    "        'Depth_to_Groundwater_Pozzo_3', 'Depth_to_Groundwater_Pozzo_4',\n",
    "        'Temperature_Siena_Poggio_al_Vento', 'Temperature_Mensano', 'Temperature_Pentolina',\n",
    "        'Temperature_Monteroni_Arbia_Biena', 'Volume_Pozzo_1', 'Volume_Pozzo_3',\n",
    "        'Volume_Pozzo_4'\n",
    "    ], [ 'Depth_to_Groundwater_Podere_Casetta' ])\n",
    "\n",
    "petrignano = AquiferDataset(\n",
    "    '../data/raw/aquifer/petrignano.csv', [\n",
    "        'Rainfall_Bastia_Umbra', 'Temperature_Bastia_Umbra',\n",
    "        'Temperature_Petrignano', 'Volume_C10_Petrignano',\n",
    "        'Hydrometry_Fiume_Chiascio_Petrignano'\n",
    "    ], [\n",
    "        'Depth_to_Groundwater_P24', 'Depth_to_Groundwater_P25'\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is more about the general pipeline than the individual model; our dummy model, then, will always predict the number 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlwaysZeroPredictor:\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self._outshape = 1 if len(y.shape) == 1 else y.shape[1]\n",
    "        self._onedim = self._outshape == 1\n",
    "        return self\n",
    "    \n",
    "    def predict(self, n):\n",
    "        return np.zeros((n, self._outshape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeSeriesValidation:\n",
    "    \n",
    "    def __init__(self, d):\n",
    "        self.log = []\n",
    "        self.d = d\n",
    "    \n",
    "    def split(self, X, y):\n",
    "        self.k = math.floor(X.shape[0] / self.d - 1)\n",
    "        self.tscv = TimeSeriesSplit(self.k)\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        return self\n",
    "    \n",
    "    def validate(self, m):\n",
    "        for tr, tt in self.tscv.split(self.X, self.y):\n",
    "            X_train, X_test = self.X.iloc[tr], self.X.iloc[tt]\n",
    "            y_train, y_test = self.y.iloc[tr], self.y.iloc[tt]\n",
    "            m.fit(X_train, y_train)\n",
    "            y_pred = pd.DataFrame(\n",
    "                m.predict(y_test.shape[0]),\n",
    "                columns = y_test.columns,\n",
    "                index = y_test.index\n",
    "            )\n",
    "            rmse = mean_squared_error(y_test, y_pred, squared=False, multioutput='raw_values')\n",
    "            mae = mean_absolute_error(y_test, y_pred, multioutput='raw_values')\n",
    "            self.log.append({\n",
    "                'train': tr,\n",
    "                'test': tt,\n",
    "                'model': m,\n",
    "                'predictions': y_pred,\n",
    "                'rmse': rmse,\n",
    "                'mae': mae\n",
    "            })\n",
    "        return self\n",
    "    \n",
    "    def report(self):\n",
    "        rmse = [x['rmse'] for x in self.log]\n",
    "        rmse = {c: np.mean([x[i] for x in rmse]) for i, c in enumerate(self.y.columns)}\n",
    "        mae = [x['mae'] for x in self.log]\n",
    "        mae = {c: np.mean([x[i] for x in mae]) for i, c in enumerate(self.y.columns)}\n",
    "        rmse['mean'] = np.mean(list(rmse.values()))\n",
    "        mae['mean'] = np.mean(list(mae.values()))\n",
    "        rmse['metric'] = 'rmse'\n",
    "        mae['metric'] = 'mae'\n",
    "        return rmse, mae\n",
    "    \n",
    "    def plot(self, body):\n",
    "        rmses, maes = self.report()\n",
    "        fig, axs = plt.subplots(\n",
    "            len(self.y.columns),\n",
    "            1,\n",
    "            figsize=(20, 5 * len(self.y.columns))\n",
    "        )\n",
    "        if len(self.y.columns) == 1:\n",
    "            axs = [axs]\n",
    "        fig.suptitle(f\"{body} Aquifer Predictions | RMSE {rmses['mean']} ... MAE {maes['mean']}\", fontweight='bold')\n",
    "        preds = [x['predictions'] for x in self.log]\n",
    "        for i, c in enumerate(self.y.columns):\n",
    "            axs[i].scatter(self.y.index, self.y[c], s=1, alpha=0.75)\n",
    "            axs[i].set_title(f\"{c} | RMSE {rmses[c]} ... MAE {maes[c]}\")\n",
    "            for p in preds:\n",
    "                axs[i].scatter(p.index, p[c].values, s=1, c='C3', alpha=0.25)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this interval\n",
    "# this model\n",
    "# these datasets\n",
    "\n",
    "class HeterogeneousValidation:\n",
    "    \n",
    "    def __init__(self, d, m, *ads):\n",
    "        \n",
    "        self.tsvs = []\n",
    "        for ad in ads:\n",
    "            tsv = TimeSeriesValidation(d)\n",
    "            tsv.split(ad.df[ad.X], ad.df[ad.y])\n",
    "            tsv.validate(m)\n",
    "            self.tsvs.append(tsv)\n",
    "    \n",
    "    def verbose(self):\n",
    "        pass\n",
    "    \n",
    "    def concise(self):\n",
    "        rmses = [tsv.report()[0]['mean'] for tsv in self.tsvs]\n",
    "        maes = [tsv.report()[1]['mean'] for tsv in self.tsvs]\n",
    "        return np.mean(rmses), np.mean(maes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "hv = HeterogeneousValidation(\n",
    "    7,\n",
    "    AlwaysZeroPredictor(),\n",
    "    auser, doganella, luco, petrignano\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LET IT HERE BE KNOWN\n",
    "\n",
    "When using the method of \"always predict 0\" and intervals of 7 days, our scores are such:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8350535491577347, 0.8258548852105945)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hv.concise() # RMSE, MAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "acea-KYwgFBtm-py3.8",
   "language": "python",
   "name": "acea-kywgfbtm-py3.8"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
